[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python Computing Supplement",
    "section": "",
    "text": "Preface\nThis is a computing supplement to the main website that uses python, and in particular scikit-learn for modeling. The structure is similar to the website, but the content here shows how to use this software for each topic.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Python Computing Supplement",
    "section": "License",
    "text": "License\n\nAs this computing supplement will largely be adapting the book, we adopt here the same license, CC BY-NC-SA 4.0",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#intended-audience",
    "href": "index.html#intended-audience",
    "title": "Python Computing Supplement",
    "section": "Intended Audience",
    "text": "Intended Audience\nReaders should have used python before, but do not have to be experts. If you are new to python, we suggest taking a look at the Python Data Science Handbook.\nYou do not have to be a modeling expert either. We hope that you have used a linear or logistic regression before and understand basic statistical concepts such as correlation, variability, probabilities, etc.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-can-i-ask-questions",
    "href": "index.html#how-can-i-ask-questions",
    "title": "Python Computing Supplement",
    "section": "How can I ask questions?",
    "text": "How can I ask questions?\nIf you have questions about the content, it is probably best to ask on a public forum, like Stack Overflow for programmatic questions, or the data science or statistics Stack Exchange sites. You’ll most likely get a faster answer there if you take the time to ask the questions in the best way possible.\nIf you want a direct answer from us, you should follow what Max calls Yihui’s Rule: add an issue to GitHub (labeled as “Discussion”) first. It may take some time for us to get back to you.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#can-i-contribute",
    "href": "index.html#can-i-contribute",
    "title": "Python Computing Supplement",
    "section": "Can I contribute?",
    "text": "Can I contribute?\nThere is a contributing page with details on how to get up and running to compile the materials and suggestions on how to help.\nIf you just want to fix a typo, you can make a pull request to alter the appropriate .qmd file.\nPlease feel free to improve the quality of this content by submitting pull requests.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#computing-notes",
    "href": "index.html#computing-notes",
    "title": "Python Computing Supplement",
    "section": "Computing Notes",
    "text": "Computing Notes\nQuarto version 1.4.538 was used to compile and render the materials.\n\nPython version 3.11.7 was used for computations. For the list of python packages used and their versions, see the Pipfile in the source repository.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/contributing.html",
    "href": "chapters/contributing.html",
    "title": "Contributing",
    "section": "",
    "text": "Software",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/contributing.html#software",
    "href": "chapters/contributing.html#software",
    "title": "Contributing",
    "section": "",
    "text": "Quarto\nQuarto is an open-source scientific and technical publishing system. Quarto version 1.4.538 is used to compile the website.\n\n\npython and Pipenv\nPython 3.11.7 is what we are currently using. There are several IDEs that you can use. We’ve used VSCode. \nI’ve set this up with Pipenv as an environment manager, but may switch.\nInstall pipenv if you don’t already have it. From the project top-level folder, run from a command line pipenv install. This may take some time, as it creates and populates an environment with all the required packages.\n\n\nBuilding the site\nFrom your pipenv (e.g. by starting in the root folder and running pipenv shell), run quarto render. This should run all the python snippets (using the pipenv python), caching some of the results, and render the output into /docs for you to preview. (For now, the github pages are also served directly from /docs, but eventually we will probably set up github actions to generate the docs on PR merges, at which point the local copy will be added to .gitignore and just for your preview.)",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/introduction.html",
    "href": "chapters/introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "To run the code in this supplement, you’ll need python and a handful of packages. It’s never too early to start using python environments; for the purposes of this supplement, anaconda is probably easiest, as it comes with most of the packages you’ll need. (If you need to save some disk space, consider miniconda, and install the additional packages as you go, or from the Pipfile in the github repository. Or, if you prefer, use pipenv directly. Or some other python environment manager of your choice.)\nOccasionally (and increasingly as this ages) you may find version issues. If so, check the Pipfile in the github repository for the versions being used at the time this page was rendered. Either install those specific versions of packages, or use that to track down what may have changed with the versions you already have, and modify the code accordingly. (And consider making a PR for the update!)\nWhen running code, use ipython or jupyter. All imports will be made explicitly in the chapters.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html",
    "href": "chapters/whole-game.html",
    "title": "2  The Whole Game",
    "section": "",
    "text": "2.1 Load the Data\nStart by loading the data;1 pandas is the standard dataframe package in python.2\nimport pandas as pd\ndeliveries = pd.read_csv(\"data/deliveries.csv\", index_col=0)\ndeliveries.head()\ntime_to_delivery\nhour\nday\ndistance\nitem_01\nitem_02\nitem_03\nitem_04\nitem_05\nitem_06\n...\nitem_18\nitem_19\nitem_20\nitem_21\nitem_22\nitem_23\nitem_24\nitem_25\nitem_26\nitem_27\n\n\n\n\n1\n16.1106\n11.899\nThu\n3.15\n0\n0\n2\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n22.9466\n19.230\nTue\n3.69\n0\n0\n0\n0\n0\n0\n...\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n30.2882\n18.374\nFri\n2.06\n0\n0\n0\n0\n1\n0\n...\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n\n\n4\n33.4266\n15.836\nThu\n5.97\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n5\n27.2255\n19.619\nFri\n2.52\n0\n0\n0\n1\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n\n5 rows × 31 columns\npandas provides plotting utilities, wrapping matplotlib:\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfig, ax = plt.subplots(1, 2, sharey=True)\ndeliveries['time_to_delivery'].hist(bins=30, ax=ax[0], label=\"time to delivery\")\ndeliveries['time_to_delivery'].apply(np.log).hist(bins=30, ax=ax[1], label=\"log(time to delivery)\")\nax[0].set_xlabel(\"time_to_delivery\")\nax[1].set_xlabel(\"log(time_to_delivery)\")\nax[0].set_ylabel(\"count\")\nplt.show();\n\n\n\n\n\n\n\nFigure 2.1\nReading from a CSV can’t intuit everything we’d like. The day column, the day of the week, was loaded as strings. It’ll be helpful in some places to cast them to a categorical type.\ndeliveries['day'] = (\n    deliveries['day']\n    .astype('category')\n    .cat.set_categories(\n        ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'],\n        ordered=True,\n    )\n)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#data-spending",
    "href": "chapters/whole-game.html#data-spending",
    "title": "2  The Whole Game",
    "section": "2.2 Data Spending",
    "text": "2.2 Data Spending\nsklearn provides train_test_split for simple splits, and several other cross-validation generators.\ntrain_test_split doesn’t currently support stratifying on a continuous outcome. It’s probably not really needed here: with a large dataset randomness will generally work just fine. But we can stratify on the binned outcome3:\n\nfrom sklearn.model_selection import train_test_split\ndelivery_train_val, delivery_test = train_test_split(\n    deliveries,\n    test_size=0.2,\n    random_state=42,\n    stratify=pd.qcut(deliveries['time_to_delivery'], 4),\n)\ndelivery_train, delivery_val = train_test_split(\n    delivery_train_val,\n    test_size=0.2 / 0.8,\n    random_state=42,\n    stratify=pd.qcut(delivery_train_val['time_to_delivery'], 4),\n)\n\nprint(len(delivery_train), len(delivery_val), len(delivery_test))\n\n6006 2003 2003",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#exploratory-data-analysis",
    "href": "chapters/whole-game.html#exploratory-data-analysis",
    "title": "2  The Whole Game",
    "section": "2.3 Exploratory Data Analysis",
    "text": "2.3 Exploratory Data Analysis\n\n2.3.1 Distance and datetime features\nPlotting smoothed trendlines isn’t so easy in pandas+matplotlib4; for now we avoid additional packages, relying on just the scatter plots in the top charts and adding a binned trend line for hour-vs-day interaction.\n\nfrom matplotlib import pyplot as plt\n\nfig, ax = plt.subplots(2, 2, figsize=(8, 6))\n\ndelivery_train.plot.scatter(\n    x='distance',\n    y='time_to_delivery',\n    alpha=0.1,\n    ax=ax[0, 0],\n)\n\ndelivery_train.plot.scatter(\n    x='hour',\n    y='time_to_delivery',\n    alpha=0.1,\n    ax=ax[0, 1],\n)\nax[0, 1].set_ylabel('')\n\ndelivery_train.boxplot(\n    column='time_to_delivery',\n    by='day',\n    ax=ax[1, 0],\n)\nax[1, 0].set_title('')\n\n\n# without fitting smoothers (another plotting package would help here),\n# we'll bin the `hour` per `day` and line-plot the mean target\ntemp = delivery_train.copy()\ntemp['hour_bin'] = (\n    temp['hour']\n    .transform(pd.qcut, q=8)\n    .apply(lambda x: x.mid)\n    .astype('float')\n)\ngrouped = (\n    temp\n    .groupby(['day', 'hour_bin'], observed=True)\n    ['time_to_delivery']\n    .mean()\n    .reset_index('hour_bin')\n    .groupby('day', observed=True)\n)\nfor day, data in grouped:\n    data.plot.line(x='hour_bin', y='time_to_delivery', label=day, ax=ax[1, 1])\nplt.legend()\n\nplt.tight_layout()\nplt.suptitle('EDA plots')\nplt.show()\n\n\n\n\n\n\n\nFigure 2.2\n\n\n\n\n\n\n\n2.3.2 Bootstrap confidence intervals for item effects\nDefine the metric, make bootstrap samples and apply the metric:\n\ndef rel_increase_time_item(df, col):\n    \"\"\"Computes the relative increase to delivery time when\n    the item for column `col` is present.\"\"\"\n    return (\n        df[['time_to_delivery']]\n        .groupby(df[col] &gt; 0)\n        .mean()\n        .apply(lambda x: x[True] / x[False] - 1)\n        .item()\n    )\n\nresample_stats = []\nfor _ in range(1001):\n    resample = delivery_train.sample(frac=1, replace=True)\n    stat = {}\n    for col in [col for col in resample.columns if col[:5] == \"item_\"]:\n        stat[col] = rel_increase_time_item(resample, col)\n    resample_stats.append(stat)\nresample_stats = pd.DataFrame(resample_stats)\nresample_stats.head()\n\n\n\n\n\n\n\n\nitem_01\nitem_02\nitem_03\nitem_04\nitem_05\nitem_06\nitem_07\nitem_08\nitem_09\nitem_10\n...\nitem_18\nitem_19\nitem_20\nitem_21\nitem_22\nitem_23\nitem_24\nitem_25\nitem_26\nitem_27\n\n\n\n\n0\n0.059837\n0.027667\n0.016086\n0.012144\n0.007851\n0.018182\n0.026414\n0.006065\n0.030333\n0.046600\n...\n0.011364\n-0.038378\n0.020940\n0.028779\n0.019817\n0.015285\n0.044199\n0.020167\n0.046261\n0.063177\n\n\n1\n0.055689\n0.018290\n-0.005845\n-0.004415\n0.005912\n0.010217\n0.015990\n-0.004823\n0.028485\n0.058809\n...\n0.000547\n-0.033938\n0.024252\n0.012355\n0.045347\n0.025344\n0.026585\n0.014412\n0.027174\n0.047914\n\n\n2\n0.045958\n0.018760\n-0.005201\n-0.007170\n-0.006138\n0.014871\n0.014589\n0.010838\n0.020212\n0.059233\n...\n0.007720\n-0.032272\n0.025981\n0.016507\n0.034392\n0.036563\n0.026727\n0.018097\n0.027168\n0.063526\n\n\n3\n0.056362\n0.016948\n0.002318\n0.008138\n-0.004072\n0.022160\n0.028072\n-0.002906\n0.043547\n0.052030\n...\n0.020688\n-0.041503\n0.023177\n0.014446\n0.041065\n0.010789\n0.054260\n0.010386\n0.039690\n0.059420\n\n\n4\n0.052518\n0.008144\n0.011398\n-0.000210\n0.005164\n0.015617\n0.023016\n0.015322\n0.028712\n0.059072\n...\n0.009432\n-0.033609\n0.009862\n0.029123\n0.020962\n0.049653\n0.016106\n0.023727\n0.024121\n0.033796\n\n\n\n\n5 rows × 27 columns\n\n\n\nDefine the confidence intervals:\n\nci = resample_stats.apply(np.percentile, q=[5, 95])\nci.index = ['lower', 'upper']\n\nci = ci.T\nci['sample'] = [\n    rel_increase_time_item(delivery_train, col)\n    for col in delivery_train.columns if col[:5] == \"item_\"\n]\nci = ci.sort_values('sample')\n\nPlot:\n\nfig = plt.figure(figsize=(5, 12))\nfor y, (col, stats) in enumerate(ci.iterrows()):\n    plt.plot([stats['lower'], stats['upper']], [y, y], c='b')\n    plt.plot(stats['sample'], y, 'bo')\nplt.axvline(0, ls='--', c='r', alpha=0.2)\nplt.yticks(np.arange(len(ci)), labels=ci.index)\nplt.xlabel(\"Increase in delivery time when ordered\")\n\nplt.show();\n\n\n\n\n\n\n\nFigure 2.3",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#model-development",
    "href": "chapters/whole-game.html#model-development",
    "title": "2  The Whole Game",
    "section": "2.4 Model Development",
    "text": "2.4 Model Development\nsklearn uses separate parameter slots for its independent and dependent variables, so\n\ny_var = 'time_to_delivery'\nX_train = delivery_train.drop(columns=y_var)\nX_val = delivery_val.drop(columns=y_var)\nX_test = delivery_test.drop(columns=y_var)\ny_train = delivery_train[y_var]\ny_val = delivery_val[y_var]\ny_test = delivery_test[y_var]\n\n\n2.4.1 Linear model\nWe use sklearn’s OneHotEncoder to produce indicator columns (a.k.a. dummy variables, one-hot encoding) for the day variable. (pandas also has make_dummies, but this requires more work with the validation and test sets (and production), so we prefer to keep everything in sklearn.)\nFor splines, we have SplineTransformer.\nFor interaction terms, there’s not a direct sklearn transformer. We’ll use a FunctionTransformer and define the transformation function directly.5\nTo apply preprocessors to different subsets of columns, we use ColumnTransformer.\n\nfrom sklearn import set_config\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, SplineTransformer, FunctionTransformer\n\nset_config(transform_output=\"pandas\")\n\nohe = OneHotEncoder(\n    sparse_output=False,\n    handle_unknown='ignore',\n)\nspl = SplineTransformer(knots='quantile')\n\ndef interactions(X):\n    for day_col in [col for col in X.columns if col[:4] == 'day_']:\n        for hour_basis in [col for col in X.columns if col[:5] == 'hour_']:\n            X[f'{day_col}*{hour_basis}'] = X[day_col] * X[hour_basis]\n    return X\n\nint_tfm = FunctionTransformer(interactions, check_inverse=False)\n\npreproc_lr = Pipeline([\n    ('step_1', ColumnTransformer(\n        [\n            ('ohe', ohe, ['day']),\n            ('spl', spl, ['hour']),\n        ],\n        remainder='passthrough',\n        verbose_feature_names_out=False,\n        )\n    ),\n    ('interact', int_tfm),\n])\n\npreproc_lr\n\nPipeline(steps=[('step_1',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('ohe',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  ['day']),\n                                                 ('spl',\n                                                  SplineTransformer(knots='quantile'),\n                                                  ['hour'])],\n                                   verbose_feature_names_out=False)),\n                ('interact',\n                 FunctionTransformer(check_inverse=False,\n                                     func=&lt;function interactions at 0x000001BC4DA0AFC0&gt;))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('step_1',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('ohe',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  ['day']),\n                                                 ('spl',\n                                                  SplineTransformer(knots='quantile'),\n                                                  ['hour'])],\n                                   verbose_feature_names_out=False)),\n                ('interact',\n                 FunctionTransformer(check_inverse=False,\n                                     func=&lt;function interactions at 0x000001BC4DA0AFC0&gt;))])step_1: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('ohe',\n                                 OneHotEncoder(handle_unknown='ignore',\n                                               sparse_output=False),\n                                 ['day']),\n                                ('spl', SplineTransformer(knots='quantile'),\n                                 ['hour'])],\n                  verbose_feature_names_out=False)ohe['day']OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse_output=False)spl['hour']SplineTransformerSplineTransformer(knots='quantile')remainderpassthroughpassthroughFunctionTransformerFunctionTransformer(check_inverse=False,\n                    func=&lt;function interactions at 0x000001BC4DA0AFC0&gt;)\n\n\nAll sklearn estimators (both transformers and model objects) implement a fit method that learns statistics/parameters from the data. Transformers provide transform for applying their transformations to data (whether training or test data; for training data, fit_transform is available and most often means just fit then transform). Model objects provide predict (and probabilistic classifiers provide predict_proba). There are many other methods and attributes, but these will get us through the rest of this chapter.\nLet’s see what the fully preprocessed data looks like.\n\npreproc_lr.fit_transform(X_train)\n\n\n\n\n\n\n\n\nday_Fri\nday_Mon\nday_Sat\nday_Sun\nday_Thu\nday_Tue\nday_Wed\nhour_sp_0\nhour_sp_1\nhour_sp_2\n...\nday_Tue*hour_sp_4\nday_Tue*hour_sp_5\nday_Tue*hour_sp_6\nday_Wed*hour_sp_0\nday_Wed*hour_sp_1\nday_Wed*hour_sp_2\nday_Wed*hour_sp_3\nday_Wed*hour_sp_4\nday_Wed*hour_sp_5\nday_Wed*hour_sp_6\n\n\n\n\n5732\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4480\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.006219\n0.294690\n0.612966\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n7337\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.015240\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n8762\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n931\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.008700\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2742\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.035889\n0.480137\n0.465214\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n8232\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.000036\n0.120192\n0.637531\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n6660\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.002236\n0.227557\n0.640921\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n68\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.083970\n0.588468\n0.325075\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n8066\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.000000\n0.004694\n0.295272\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n6006 rows × 91 columns\n\n\n\nWe can put that frame directly into a model, or again wrap the preprocessor in a pipeline with the model (which will make predicting slightly easier, as the transformations will happen under the hood).\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\n\npipe_lr = Pipeline([\n    ('preproc', preproc_lr),\n    ('linear_reg', LinearRegression()),\n])\npipe_lr.fit(X_train, y_train)\n\ny_pred = pipe_lr.predict(X_val)\nmean_absolute_error(y_true=y_val, y_pred=y_pred)\n\n1.6421489340208437\n\n\nTake a look at a calibration plot, the actual-vs-predicted values:\n\nfrom sklearn.metrics import PredictionErrorDisplay\nPredictionErrorDisplay.from_predictions(\n    y_true=y_val,\n    y_pred=y_pred,\n    kind='actual_vs_predicted',\n    scatter_kwargs={'alpha': 0.1},\n)\n\n\n\n\n\n\n\nFigure 2.4\n\n\n\n\n\n\n\n2.4.2 Random forest\nsklearn doesn’t have a model-based recursive partitioning like cubist; there is a different python package just for that, but to stick to sklearn for now let’s fit instead a random forest. Random forests build binary trees like cubist, but with constant predictions from each leaf instead of the linear models that cubist produces. To reach similar performance then, we’ll want deeper trees.\nAs in the book, this tree-based model doesn’t necessitate as much transformation to perform well. However, at time of writing sklearn’s random forest doesn’t handle categorical features, so we’ll need to one-hot encode day still.\nAnd intuitively the number of items ought to also be important; while the linear model gets that for free (just the sum of the item_ columns), and a tree can approximate it arbitrarily closely, it can be beneficial to the learning procedure to expose this as a feature directly. We can add that in the pipeline as another FunctionTransformer.\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.compose import make_column_selector\n\nrf = RandomForestRegressor(\n    max_depth=15,\n    n_estimators=100,\n    random_state=42,\n)\n\ndef item_count(X):\n    X['item_count'] = X.sum(axis=1)\n    return X\n\npreproc_rf = ColumnTransformer(\n    [\n        ('ohe', ohe, ['day']),\n        ('items', FunctionTransformer(item_count), make_column_selector(pattern='item_*')),\n    ],\n    remainder='passthrough',\n    verbose_feature_names_out=False,\n)\n\npipe_rf = Pipeline([\n    ('preproc', preproc_rf),\n    ('rand_forest', rf),\n])\n\npipe_rf.fit(X_train, y_train)\n\ny_pred = pipe_rf.predict(X_val)\nmean_absolute_error(y_true=y_val, y_pred=y_pred)\n\n1.4992085511625766\n\n\nAdding the item count appears (on the validation set) to have helped a bit. Taking into account our analysis of item presence, we might modify the count to exclude item 19, or increase the weight on item 10, etc. But at that point the trees might already be making the relevant modifications.\n\n\n2.4.3 Neural network\nsklearn isn’t the best package for neural networks, but it does provide a simple implementation:\n\nfrom sklearn.neural_network import MLPRegressor\nnn = MLPRegressor(\n    max_iter=500,\n    learning_rate_init=0.01,\n    random_state=42,\n)\n\nAs in the book, we won’t get into the large space of hyperparameters, and just tune the number of neurons in a single hidden layer. sklearn offers GridSearchCV for tuning hyperparameters in a grid style, or RandomizedSearchCV for a random search. Other packages offer other strategies.\nThe tuners in sklearn use k-fold cross-validation by default. While it’s possible to tune using a fixed validation set, we’ll just use the k-fold splitting of the training set to score hyperparameters, and score the best hyperparameter against our validation set.\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\n\npreproc_nn = ColumnTransformer(\n    [('ohe', ohe, ['day'])],\n    remainder=StandardScaler(),\n)\n\npipe_nn = Pipeline([\n    ('preproc', preproc_nn),\n    ('neural_net', nn),\n])\n\n# setting hyperparameters for a pipeline\n# uses &lt;step_name&gt;__&lt;parameter&gt;\nparams = {\n    'neural_net__hidden_layer_sizes': [(k,) for k in range(2, 10)],\n}\n\n# By default the search will use k-fold cross-validation on its training set.\n# If we want to score on the already-defined validation set,\n# we need to pass train+val into `fit` and specify the validation mask in the `cv` parameter.\ndelivery_train_val\nsearch = GridSearchCV(\n    estimator=pipe_nn,\n    param_grid=params,\n    cv=[[list(range(len(X_train))), list(range(len(X_train), len(X_train) + len(X_val)))]],\n    scoring='neg_mean_absolute_error',\n    n_jobs=3,\n)\n\nsearch.fit(pd.concat([X_train, X_val]), pd.concat([y_train, y_val]))\nsearch.best_score_\n\n-1.5896399846317641\n\n\nTo find out more about the search, we have a look at the attribute cv_results_:\n\ncv_results_frame = pd.DataFrame(search.cv_results_)\ncv_results_frame\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_neural_net__hidden_layer_sizes\nparams\nsplit0_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n0\n1.961829\n0.0\n0.028263\n0.0\n(2,)\n{'neural_net__hidden_layer_sizes': (2,)}\n-2.846601\n-2.846601\n0.0\n8\n\n\n1\n3.130723\n0.0\n0.031076\n0.0\n(3,)\n{'neural_net__hidden_layer_sizes': (3,)}\n-2.810811\n-2.810811\n0.0\n7\n\n\n2\n7.660357\n0.0\n0.029812\n0.0\n(4,)\n{'neural_net__hidden_layer_sizes': (4,)}\n-1.799978\n-1.799978\n0.0\n5\n\n\n3\n4.921726\n0.0\n0.026736\n0.0\n(5,)\n{'neural_net__hidden_layer_sizes': (5,)}\n-2.648188\n-2.648188\n0.0\n6\n\n\n4\n3.323614\n0.0\n0.026697\n0.0\n(6,)\n{'neural_net__hidden_layer_sizes': (6,)}\n-1.763588\n-1.763588\n0.0\n4\n\n\n5\n4.643244\n0.0\n0.026805\n0.0\n(7,)\n{'neural_net__hidden_layer_sizes': (7,)}\n-1.677239\n-1.677239\n0.0\n3\n\n\n6\n5.286923\n0.0\n0.021809\n0.0\n(8,)\n{'neural_net__hidden_layer_sizes': (8,)}\n-1.602884\n-1.602884\n0.0\n2\n\n\n7\n5.278420\n0.0\n0.021161\n0.0\n(9,)\n{'neural_net__hidden_layer_sizes': (9,)}\n-1.589640\n-1.589640\n0.0\n1\n\n\n\n\n\n\n\nPlotting from that dataframe:\n\n# extract the numeric hidden layer size\n# from the tuple-typed hyperparameter\ncv_results_frame['hidden_layer_size'] = cv_results_frame[\n    'param_neural_net__hidden_layer_sizes'\n].apply(lambda x: x[0])\n\n# convert back from scorer neg_mae to mae\ncv_results_frame['mae'] = - cv_results_frame['mean_test_score']\n\nfrom matplotlib import colormaps\nfig, ax = plt.subplots(1)\ncv_results_frame.plot.scatter(\n    x='hidden_layer_size',\n    y='mae',\n    ax=ax,\n)\nplt.legend()\nax.set_ylabel(\"MAE\")\nplt.show();\n\n\n\n\n\n\n\nFigure 2.5",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#footnotes",
    "href": "chapters/whole-game.html#footnotes",
    "title": "2  The Whole Game",
    "section": "",
    "text": "We’ve opted to use the comma-separated values (CSV) format because that’s easily loaded in many programs; there are other formats that hold more information (like column data types) and are more efficient.↩︎\n but again, fancier things exist, perhaps most notably polars↩︎\n we use quartiles to match tidymodels, see docs↩︎\n the latter have expressed concern over making up information as well as possibly exploding the number of parameters needed for different uses↩︎\n We could also use PolynomialFeatures here, with degree 2 and interaction_only=True to prevent \\(\\mathrm{feature}^2\\) terms; that would include things like interactions of Monday with Wednesday, or two spline bases, which we could probably clean up downstream, but this will be a little nicer.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  }
]